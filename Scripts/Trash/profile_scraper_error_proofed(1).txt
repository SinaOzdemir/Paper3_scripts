#Profile downloader
#to do:
#it doesnt revert back to the last iteration, thus doesnt retry the failed iteration
#wrap get_* functions in tryCatch and insert withRestrat in the expression
#so that when there is an error 1)it saves the last iteration,2)sleeps for 10 min,3)retries the last itteration
#use foreach instead of for and while
#


#load packages
install.packages("pacman")
library(pacman)
packs = c("rtweet","tidyverse")
p_load(char = packs,character.only = T)

#create dirs

dataDIR = paste0(getwd(),"/Data/")

tweetSaveDIR = paste0(getwd(),"/Data/replier_tweets/")

followerSaveDIR =paste0(getwd(),"/Data/replier_followers/")

friendsSaveDIR = paste0(getwd(),"/Data/replier_friends/")

#create the twitter token

profileScraper = rtweet::create_token(app = "profile_seagull",
                                      consumer_key = "wk3YguA5RbsXakbX3Qe5kjLjn",
                                      consumer_secret = "ozmx63CxJYm9UxUHj3O9xe8IkKTBA7dhNqG5ssSdAO8EwsQ7o6",
                                      access_token = "1151438784384421888-L9PFgajVGBO6bjLNl5upvprwRgL9YU",
                                      access_secret = "iKQey4QtaOpGLahFuvHEieVyQMSyos9uXmQ8decWjdyMc",
                                      set_renv = F)

#create files

prof_to_download<- readRDS(file = paste0(dataDIR,"prof_download.RDS"))

replierIDlist = split(prof_to_download, ceiling(seq_along(prof_to_download)/1000))

replierProfileTWEETS = vector("list",length = 1000)

replierProfileFOLLOWERS = vector("list",length = 1000)

repliersProfileFRIENDS = vector("list", length = 1000)



for (k in 1:length(replierIDlist)){
  print(paste0("starting the chunk", k," out of ", length(replierIDlist)))
  
  replierIDunlist = unlist(replierIDlist[[k]])
  
  j<-1
  
  nextChunkCond<- length(replierIDunlist)
  #I use while to be able to restart the scraping process
  #when there is an error. The logic is while j is smaller
  #than the length of user id vector, keep downloading the profiles,
  #after each download increase j by 1
  #tryCatch interrupts this process in case of an error
  #and sets j back to j-1 so that the while loop can try to scrape jth profile again
  
  
  while (j<=nextChunkCond) {
    
    print(paste0("Scraping the tweets of ", j,"th replier "))
    
    #if condition to accommodate the api limits
    rl = rate_limits(token = profileScraper)
    
    rl_tweets = filter(rl, query %in% c("application/rate_limit_status","statuses/user_timeline"))
    #I am giving some elbow room to query limit so as not to get nasty warnings
    if (isTRUE(rl_tweets[1,3]<=2 | rl_tweets[2,3]<= 2)){
      print(paste0(j," is out of limit and waiting for 15 minutes at", Sys.time()))
    
      Sys.sleep((15*60))
    }
    
    #here, I used tryCatch to defend against possible errors
    #it saves the scraped profiles then forces the system to sleep 10 min before reseting j to j-1 in case of an error so that while loop can retry jth user
    replierProfileTWEETS[[j]] = tryCatch(expr = get_timeline(user = replierIDunlist[j],
                                                             n = 3200,parse = T,token = profileScraper),
                                         warning = function(w){
                                           message(w)},
                                         error = function(e){
                                           print(paste0("something has gone pearshaped with scraping tweets of",j,"th user!\ninitiating defensive protocol"))
                                           saveRDS(object = replierProfileTWEETS,
                                                   file = paste0(tweetSaveDIR,"user_tweets_erro_r",j,".RDS"))
                                           Sys.sleep(15*60)
                                           j<-j-1
                                           return(NULL)
                                         },
                                         finally = {
                                           message(paste0("something has gone wrong with ",j,"th user's tweets"))
                                         })
    print(paste0(j,"th user's tweets are scraped, moving on to their followers"))
    #saves the scraped data every 1000 user
    #if j == stop condition?
    
    if (isTRUE(j == nextChunkCond)) {
      
      print(paste0("saving the tweets from",k,"th batch"))
      
      saveRDS(object = replierProfileTWEETS,file = paste0(tweetSaveDIR,"user_tweets_",k,".RDS"))
      
      remove(replierProfileTWEETS)
      
      replierProfileTWEETS = vector("list",length = 1000)
    }
    
    #rinse and repeat to scrape jth users followers
    print(paste0("scraping the followers of ",j,"th replier"))
    
    rl_follower = filter(rl,query%in% c("application/rate_limit_status","followers/ids"))
    
    
    if (isTRUE(rl_follower[1,3] <= 2 | rl_follower[2,3] <= 2)) {
      
      print(x = paste0("Follower's ",j," is out of limit and waiting for 15 minutes at ", Sys.time()))
      
      Sys.sleep(time = (15*60))}
    
    user<-lookup_users(users = replierIDunlist[j],parse = T,token = profileScraper)
    
    if(user$followers_count <=5000){
    replierProfileFOLLOWERS[[j]] = tryCatch(expr = get_followers(user = replierIDunlist[j],
                                                                 n = 5000,parse = T,
                                                                 verbose = F,
                                                                 token = profileScraper),
                                            warning = function(w){
                                              message(w)},
                                            error = function(e){
                                              print(paste0("something has gone pearshaped with scraping the followers of ", j,"th user! initiating defensive protocol"))
                                              saveRDS(object = replierProfileFOLLOWERS,
                                                      file = paste0(followerSaveDIR,"user_followers_error_",j,".RDS"))
                                              Sys.sleep(10*60)
                                              j<-j-1
                                              return(NULL)
                                            },
                                            finally = {
                                              message(paste0(j,"th user's followers are scraped, moving on to their friends"))
                                            })}
    else{print(paste0(j,"th user has more than 5000 followers"))
                                              large_user<-replierIDunlist[j]
                                              write(x = large_user,file = paste0(dataDIR,"central_repliers.txt"),append = T)
                                            }
    #again save follower lists every 1000 user
    #each element of a list contains the follower ids of jth user
    #for example replierProfileFOLLOWERS[1] will have the followers of replierIDunlist[1]
    if (isTRUE(j == nextChunkCond)) {
      
      print(paste0("saving the followers of ", j, "th profile"))
      
      saveRDS(object = replierProfileFOLLOWERS, file = paste0(followerSaveDIR,"user_followers_",j,"_",".RDS"))
      
      remove(replierProfileFOLLOWERS)
      
      replierProfileFOLLOWERS = vector("list",length = 1000)
      
    }
    
    #rinse and repeat to scrap jth users friends
    
    print(paste0("scraping the friends of ",j,"th replier"))
    
    rl_friends = filter(rl, query %in%c("application/rate_limit_status","friends/ids"))
    
    
    if (isTRUE(rl_friends[1,3] <= 2 | rl_friends[2,3] <= 2)) {
      
      print(x = paste0("Follower's ",j," is out of limit and waiting for 15 minutes at ", Sys.time()))
      
      Sys.sleep(time = (15*60))}
    
    # same problem as followers
    
    repliersProfileFRIENDS[[j]] = tryCatch(expr = rtweet::get_friends(user = replierIDunlist[j],
                                                                      n = 5000,
                                                                      parse = T,
                                                                      verbose = F,
                                                                      token = profileScraper),
                                           warning = function(w){
                                             message(w)},
                                           error = function(e){
                                             print(paste0("something has gone pearshaped with scraping friends of",j,"th user! initiating defensive protocol"))
                                             saveRDS(object = repliersProfileFRIENDS,
                                                     file = paste0(friendsSaveDIR,"user_friends_error_",j,".RDS"))
                                             Sys.sleep(10*60)
                                             j<-j-1
                                             return(NULL)
                                           },
                                           finally = {
                                             message(paste0(j,"th user's friends are scraped"))
                                             
                                           })
      
    ##save user friends in intervals of 1000 users, then remove the object for next 1000 so that it doesn't consume the memory
    
    if (isTRUE(j == nextChunkCond)) {
      
      print(paste0("saving the friends of ", j, "th profile"))
      
      saveRDS(object = repliersProfileFRIENDS, file = paste0(friendsSaveDIR,"user_friends_",j,"_",".RDS"))
      
      remove(repliersProfileFRIENDS)
      
      repliersProfileFRIENDS = vector("list",length = 1000)
      
    }
    print(paste0(j,"th user is complete, moving on to",j+1,"th user"))
    j<-j+1
    }
    }


